
<!DOCTYPE html>
<html lang="en">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
  
  
    <meta name="keywords" content="DFS," />
  

  
    <meta name="description" content="A Comparative Analysis of Distributed File Systems: GFS, HDFS, and Ceph" />
  
  
  <link rel="icon" type="image/x-icon" href="/logo.png">
  <title>A Comparative Analysis of Distributed File Systems: GFS, HDFS, and Ceph [ Blog ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  <div class="nav-container">
    <nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    <img class="avatar" src="https://timesea05.github.io/assets/logo.jpg">
    <span class="title">Blog</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            <li class="pure-menu-item"><a href="/" class="pure-menu-link">Home</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">Archives</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/categories" class="pure-menu-link">Categories</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/tags" class="pure-menu-link">Tags</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/about" class="pure-menu-link">About</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/atom.xml" class="pure-menu-link">RSS</a></li>
          
      
  </ul>
   
</nav>
  </div>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        A Comparative Analysis of Distributed File Systems: GFS, HDFS, and Ceph
      </h1>
      <span>
        
        <time class="time" datetime="2023-10-08T00:00:00.000Z">
        2023-10-08
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DFS/" rel="tag">DFS</a></li></ul>
      </span>
    </span>

    </header>

    <div class="post-content">
      <p>最近一段时间在学习分布式存储系统的经典论文(GFS, HDFS, Ceph)。这篇文章对比了这三个文件系统，总结了它们的共性与特性。</p>
<h2 id="overview">1. Overview</h2>
<p>我将从下面几个角度来比较这三个分布式文件系统：</p>
<ul>
<li>Architecture</li>
<li>Metadata Storage</li>
<li>Data Storage</li>
<li>Client Operation</li>
<li>Consistency Model</li>
</ul>
<p>学习DFS当然不止上面这五个角度，除了这些还有比如每个DFS的特性等，但上面的五个角度是学习分布式文件系统的最基本的五个角度了，所以务必要掌握。</p>
<h2 id="architecture">2. Architecture</h2>
<h3 id="gfs">2.1 GFS</h3>
<p><img src="/assets/gfs-hdfs-ceph-1.png" /></p>
<p>如上图所示，一个GFS集群包括一个Master节点、多个Chunk Server以及多个Client。</p>
<p>关于GFS中的chunk：在GFS中，文件都会被分割成固定大小的chunks(通常是64MB)，每一个chunk都以Linux文件的形式，存储在chunk Server中。每一个chunk都有一个对应的64bit的chunk handle(相当于id)，读写chunk时，需要指定chunk handle 以及相应的byte range。</p>
<p>Master节点存储GFS的所有metadata，包括namespace, access control information, chunk的位置以及文件到chunk的映射。Master节点同样主导system-wide activities，比如管理chunk lease和chunk migration, 回收orphaned chunks等等。Chunk Server负责存储chunks，Clients作为应用程序的代理，读写集群的数据。</p>
<h3 id="hdfs">2.2 HDFS</h3>
<p><img src="/assets/gfs-hdfs-ceph-2.jpg" /></p>
<p>HDFS集群的架构如上图所示，由Name Node, Data Node 和 HDFS Client 三个部分组成。</p>
<p>Name Node 存储文件系统的metadata，如namespace tree，file block所在的Data Node等等。Data Node负责存储block，每一个block对应Data Node之上的两个文件：block data，以及block's metadata(checksum, generation stamp, ...etc). HDFS Clients 作为应用程序的代理，读写集群的数据。</p>
<p>HDFS paper中还提到了Checkpoint Node 和 Backup Node，这些角色都是由Name Node来充当，后面再来介绍。</p>
<p>可以看到，HDFS的架构与GFS的架构非常相似，都是典型的类主-从结构，主节点存储文件系统元数据，从节点负责存储数据。</p>
<h3 id="ceph">2.3 Ceph</h3>
<p><img src="/assets/gfs-hdfs-ceph-3.png" /></p>
<p>Ceph集群的架构如上图所示，由Clients, MDS Cluster 和 OSD Cluster组成。</p>
<p>Clients向外界暴露了接近POSIX的接口，是外界读写集群数据的代理；Metadata Cluster负责管理namespace(filenames and directories)，并存储metadata；OSD Cluster存储数据。</p>
<p>与GFS, Ceph显著不同的是，Ceph的架构并不是主-从结构，而是扁平的结构，MDS Cluster与GFS 中的Master并不相同，且在MDS Cluster和OSD Cluster内部，也没有类似主节点的结构。</p>
<h2 id="metadata-storage">3. Metadata Storage</h2>
<table>
<thead>
<tr class="header">
<th></th>
<th>GFS</th>
<th>HDFS</th>
<th>Ceph</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>update content</em> <strong>persistency</strong></td>
<td>operation log</td>
<td>journal</td>
<td>journal</td>
</tr>
<tr class="even">
<td><strong>checkpoint</strong> mechanism</td>
<td>yes</td>
<td>Checkpoint Node</td>
<td>not mentioned</td>
</tr>
<tr class="odd">
<td><em>update content</em> <strong>backup</strong></td>
<td>multi machines</td>
<td>Backup Node</td>
<td>OSD cluster</td>
</tr>
</tbody>
</table>
<p>GFS, HDFS, Ceph为了较高的响应速度，都选择将metadata 存放在内存中(in-memory cache)。当客户端请求文件的metadata时，只需要从内存中查找就可以。但数据存放在内存是十分不安全的，机器掉电之后都会导致内存数据丢失，因此必须有方法来保证metadata persistence.</p>
<p>三者采用的应该都是 journal/log + checkpoint 的机制 (ceph paper 中未提到类似checkpoint的减小journal大小的机制，但猜测具体实现中应该会存在)：</p>
<ul>
<li>每次更新metadata时，将更新内容写入持久化存储(gfs: operation log; hdfs, ceph: journal)，在机器掉电后重启时，replay journal就可以恢复掉电前的内存状态</li>
<li>文件系统可能会运行很长时间，如果不做处理，journal会变得非常大，占满整个磁盘，并且replay耗时较长。所以每隔一段时间，就要做checkpoint：读取上一次的checkpoint，将journal中的更新全部应用到内存中，得到新的内存状态后，将新的内存状态做checkpoint，写入磁盘，同时清空journal</li>
</ul>
<p>这样的机制被广泛使用，可以在redis AOF+RDB 以及raft log+snapshot 中看见类似的实现。</p>
<p>机制大概如此，但是还是存在着问题，如果log 丢失了怎么办，比如硬盘损坏？</p>
<p>三个文件系统都有着自己不同的方法实现这套机制，以及解决上述提出的新问题：</p>
<ul>
<li>GFS 中master节点有多份operation log，存放在不同的主机上。每次将更新写入operation log，只有在更新成功写入所有主机的持久介质之后才能返回</li>
<li><p>之前提到了HDFS中的CheckPoint Node和Backup Node。CheckPoint Node做的事情就是刚才提到的第2点，从NameNode中获取旧的checkpoint和journal，二者结合之后生成新的checkpoint，并清空journal，并将checkpoint返回给NameNode；Backup Node顾名思义，就是做备份的Node，当NameNode向journal写入更新时，会将同样的更新发送给Backup Node；当NameNode Fail(各种形式上的)时，Backup Node就可以挺身而出</p></li>
<li><p>Ceph 应该会在OSD cluster中保存每一个MDS的journal 备份(MDS journal是存放在MDS主机上的)。当更新写入journal之后，很快更新就会写入存放在OSD 的备份</p></li>
</ul>
<p>值得注意的是，为了提高效率，文件系统通常会将更新按照批次(batch)写入持久化截止，来提高效率，否则每次写入只写入很小的内容会浪费I/O资源。</p>
<h2 id="data-storage">4. Data Storage</h2>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 36%" />
<col style="width: 21%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>GFS</th>
<th>HDFS</th>
<th>Ceph</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Striping</td>
<td>fixed size(64 MB)</td>
<td>fixed size(128MB)</td>
<td>striping strategy(no details)</td>
</tr>
<tr class="even">
<td>Snapshot</td>
<td>copy-on-write, duplicate metadata</td>
<td>duplicate hard links</td>
<td>not implemented</td>
</tr>
<tr class="odd">
<td>Placement</td>
<td>not mentioned</td>
<td>Cluster Topology</td>
<td>OSD cluster map</td>
</tr>
</tbody>
</table>
<h3 id="gfs-1">4.1 GFS</h3>
<p>前面提到了，在GFS中，文件被分成固定大小的chunks，每一个chunk的大小为64MB。Chunk Servers以Linux文件的形式存储chunk，每一个chunk都对应着一个唯一的chunk handle(id)。对chunk进行读写操作时，只需要指定chunk handle与byte range即可。每一个chunk都会有多个副本，GFS的默认策略是为3个副本，可以根据实际需求来改动replication level。</p>
<h4 id="snapshot">4.1.1 Snapshot</h4>
<p>既然是分布式存储系统，那么就肯定会有对应的Snapshot机制来保证数据的安全性。GFS使用的是与AFS类似的copy-on-write 机制来实现snapshot。</p>
<p>当master节点收到snapshot请求时，首先要做的是revoke any outstanding leases，来阻塞来自客户端的写操作；之后<em>duplicating the metadata for the source file or directory tree</em>(原句)，新创建的snapshot files 会指向与原文件相同的chunks。当写请求要写被snapshot过的chunk时，会创建一个新的chunk及其对应的chunk handle，也就是所谓的copy-on-write机制。</p>
<p>原文中的这句话<em>duplicating the metadata for the source file or directory tree</em> 有点难理解，其实这比较类似于CMU 15445 Project 0中的copy-on-write Trie，具体实现的话应该和HDFS的实现差不太多，下面会讲到。</p>
<h3 id="hdfs-1">4.2 HDFS</h3>
<p>和GFS类似，在HDFS中，每一个文件会被分割成固定大小的block(128MB)，在Data Nodes上，每一个block对应两个文件：block data和block metadata(checksums, ...etc). NameNode会存储 <em>mapping of file blocks to Data Nodes</em>。</p>
<h4 id="snapshot-1">4.2.1 Snapshot</h4>
<p>HDFS也有对应的Snapshot机制，摘录原文如下：</p>
<blockquote>
<p>Instead each DataNode creates a copy of the storage directory and hard links existing block files into it. When the DataNode removes a block it removes only the hard link, and block modifications during appends use the copy-on-write technique.</p>
</blockquote>
<p>原文中说的hard links，应该与GFS中提到的snapshot files非常类似，但目前尚未求证。</p>
<h4 id="placement-policy">4.2.2 Placement Policy</h4>
<p>为了保证数据的reliability与读写文件时的性能，HDFS提出了自己的replica placement policy:</p>
<blockquote>
<p>The default HDFS replica placement policy can be summarized as follows: 1. No Datanode contains more than one replica of any block. 2. No rack contains more than two replicas of the same block, provided there are sufficient racks on the cluster.</p>
</blockquote>
<p>HDFS Paper中提到的一个例子中，当创建一个新的block时，第一个副本首先会存放在writer 所在的DataNode，第二个、第三个副本就会分配到不同的rack中。</p>
<p><img src="/assets/gfs-hdfs-ceph-5.png" /></p>
<p>为了估计节点之间网络带宽(目的是衡量节点通信时的效率)，HDFS还提出了基于树状拓扑的办法，如上图所示。节点之间的距离就是两个节点到到根节点的距离之和，距离越短，就意味着这两个节点通信时的带宽就会越大，通信效率就越高。正因此，HDFS Client在读取数据的过程中，会优先选择从距离自己最近的Data Node读取数据。</p>
<h3 id="ceph-1">4.3 Ceph</h3>
<p><img src="/assets/gfs-hdfs-ceph-4.png" /></p>
<p>Ceph存储文件数据的方式与GFS, HDFS相差比较大。在GFS, HDFS中，文件被分成固定大小的块(chunk, block)，而在Ceph中，文件数据被条带化(striped)之后，通过一个简单的hash函数(bit mask)形成PGs(Placement Groups)后，通过CRUSH算法存储到OSD Cluster中。关于striping strategy, 论文中没有给出具体细节。</p>
<p>Ceph这样去中心化存储文件数据的好处是，不需要再为每一个文件维护 per-file block list，也不需要设定类似Master节点或者Name Node这样的节点。Client只要从MDS Cluster中获得文件的inode id，就可以通过CRUSH算法直接算出文件stripe的位置。</p>
<p>Ceph Paper中尚未提到实现和GFS, HDFS类似的Snapshot机制，在论文的结尾提到了之后会实现这样的功能。Ceph也有自己的Placement Policy，该Policy依据的是OSD cluster map，和前面提到的HDFS的树状拓扑结构比较类似，Replica的放置策略与HDFS也比较类似：</p>
<blockquote>
<p>For example, one might replicate each PG on three OSDs, all situated in the same row (to limit inter-row replication traffic) but separated into different cabinets (to minimize exposure to a power circuit or edge switch failure)</p>
</blockquote>
<h2 id="client-operation">5. Client Operation</h2>
<h3 id="gfs-2">5.1 GFS</h3>
<p><strong>Read：</strong></p>
<p>Client用固定的块大小将应用程序指定的文件名和byte range转换为文件中的chunk index。接着，Client将文件名和 byte offset发送给Master，Master返回相应的chunk handle，以及replica所在的位置(副本处于哪一台chunk server上)。Client会缓存来自Master的这些信息，并根据这些信息找到离自己最近的chunk server，向其发送请求来读取chunk 内容。</p>
<p><strong>Write：</strong></p>
<p><img src="/assets/gfs-hdfs-ceph-6.png" /></p>
<p>首先解释一下GFS中lease的概念：</p>
<blockquote>
<p>We use leases to maintain a consistent mutation order across replicas. The master grants a chunk lease to one of the replicas,which we call the primary. The primary picks a serial order for all mutations to the chunk.</p>
</blockquote>
<p>有了lease后，自然会出现续期、到期等问题，此处不再赘述，有兴趣可以到原文中查阅相关细节。</p>
<p>GFS的文件写入数据的流程大致如下：</p>
<ol type="1">
<li><p>Client 询问 Master 哪台chunk server 持有该分块的current lease，以及其他副本的位置</p></li>
<li><p>Master 返回 1中 Client 请求的信息</p></li>
<li>Client 将要写入的数据推送给该 chunk 的副本 (<strong>linear pipeline</strong>)</li>
<li>一旦所有副本都确认收到数据，客户端就会向 primary 发送写请求</li>
<li>primary 会向所有的其他副本转发写请求</li>
<li>所有其他副本向 primary 确认写入请求已经完成</li>
<li><p>primary 响应客户端，表示写入已经成功完成，否则会返回错误</p></li>
</ol>
<h3 id="hdfs-2">5.2 HDFS</h3>
<p>HDFS的读写过程与GFS几乎完全相同，此处不再赘述。下图是对5.1中提到的linear pipeline的可视化：</p>
<p><img src="/assets/gfs-hdfs-ceph-7.png" /></p>
<h3 id="ceph-2">5.3 Ceph</h3>
<p>无论读还是写，Client 都首先要向MDS cluster 请求文件名对应的 <em>file inode</em>，包含inode number, file owner, ...etc, 另外还有条带化文件数据使用的striping strategy。有了这些之后，无论我们需要文件的哪一部分(byte range)，都可以根据来自MDS cluster的striping strategy和CRUSH算法算出副本所在的OSD，从OSD cluster读写数据。</p>
<p>关于读写数据的细节，Ceph Paper中没有详细的论述，此处有待补充。</p>
<h2 id="consistency-model">6. Consistency Model</h2>
<h3 id="gfs-3">6.1 GFS</h3>
<p>GFS的Consistency Model主要是论文中的这个表格：</p>
<p><img src="/assets/gfs-hdfs-ceph-8.png" /></p>
<p>GFS用两个关键词来描述file region: <strong>consistent, defined</strong></p>
<ul>
<li><p>consistent: all clients will always see the same data, regardless of which replicas they read from</p></li>
<li><p>defined: consistent and clients will see what the mutation writes in its entirety</p></li>
</ul>
<p>下面用三张图描述表中提到的三种状态：defined, consistent but undefined, defined interspersed with inconsistent</p>
<p><img src="/assets/gfs-hdfs-ceph-9.png" /></p>
<p><img src="/assets/gfs-hdfs-ceph-10.png" /></p>
<p><img src="/assets/gfs-hdfs-ceph-11.png" /></p>
<p>其中我个人觉得理解难度比较大的是<em>defined interspersed with inconsistent</em>这种情况，在有了图示之后，就能够看的比较明白了。当Atomic Append 操作失败后，Client会重试，这样会导致duplicate和padding的出现。出现这种情况也是由于GFS Consistency Model相对Relax。解决这个问题的办法在GFS Paper中也提出了，可以使用Checksum和Unique ID来解决，GFS也提供了相应的库，让应用程序非常简单地实现这两种机制。</p>
<p>在GFS Consistency Model中，GFS保证在一系列successful mutations之后，mutated file region一定是defined而且包含最后一次写入的数据。GFS通过两种机制来保证这一条件：</p>
<ul>
<li>applying mutations to a chunk in the same order on all its replicas</li>
<li>using chunk version number to detect any replica that has become stale</li>
</ul>
<h3 id="ceph-3">6.2 Ceph</h3>
<p>Ceph实现了POSIX语义，而POSIX语义要求<em>reads reflect any data previously written, and that writes are atomic</em>。所以当一个文件被多个Client打开时，<em>the MDS will revoke any previously issued read caching and write buffering capabilities, forcing client I/O for that file to be synchronous.</em> 也就是说禁用除当前正在读写的Client以外的所有Clients的缓存功能，来使得I/O变为同步(synchronous)。</p>
<p>Synchronous I/O 显然会显著影响性能，为了解决性能问题，Ceph扩展了POSIX接口，比如为<em>open</em>操作添加了新的flag <em>O_LAZY</em>，让应用程序显示降低对共享文件读写一致性的要求。</p>

    </div>

  </article>
  <div class="toc-container">
    
  <div id="toc" class="toc-article">
    <strong class="toc-title">Posts</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#overview"><span class="toc-text">1. Overview</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#architecture"><span class="toc-text">2. Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gfs"><span class="toc-text">2.1 GFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs"><span class="toc-text">2.2 HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ceph"><span class="toc-text">2.3 Ceph</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#metadata-storage"><span class="toc-text">3. Metadata Storage</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#data-storage"><span class="toc-text">4. Data Storage</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gfs-1"><span class="toc-text">4.1 GFS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#snapshot"><span class="toc-text">4.1.1 Snapshot</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs-1"><span class="toc-text">4.2 HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#snapshot-1"><span class="toc-text">4.2.1 Snapshot</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#placement-policy"><span class="toc-text">4.2.2 Placement Policy</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ceph-1"><span class="toc-text">4.3 Ceph</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#client-operation"><span class="toc-text">5. Client Operation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gfs-2"><span class="toc-text">5.1 GFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs-2"><span class="toc-text">5.2 HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ceph-2"><span class="toc-text">5.3 Ceph</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#consistency-model"><span class="toc-text">6. Consistency Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gfs-3"><span class="toc-text">6.1 GFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ceph-3"><span class="toc-text">6.2 Ceph</span></a></li></ol></li></ol>
  </div>


  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by/4.0/">知识共享署名 4.0 国际许可协议</a>
    <span>进行许可。 转载时请注明原文链接。</span>
</div>
  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/others/oslab-guide.html" rel="next" title="UCAS OSLab Guide: Setting up the Development Environment">
          UCAS OSLab Guide: Setting up the Development Environment
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/dfs/erpc-nsdi19.html" rel="prev" title="Notes on eRPC -- NSDI&#39;19">
            Notes on eRPC -- NSDI&#39;19
          </a>
          <span>〉</span>
        
      </div>
    </div>
  


    </div>

    

  </div>
  <footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" href="https://timesea05.github.io">首页</a> |
        <a class="bottom-item" href="https://timesea05.github.io" target="_blank">主站</a> |
        <a class="bottom-item" href="https://github.com/TimeSea05" target="_blank">GitHub</a> |
        <a class="bottom-item" href="https://hexo.io" target="_blank">Powered by hexo</a> |
        <a class="bottom-item" href="https://github.com/KevinOfNeu/hexo-theme-xoxo" target="_blank">Theme xoxo</a>
    </div>
</footer>
  

<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>

  



</body>
</html>
